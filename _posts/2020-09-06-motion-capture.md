---
title: "Deep learning applications for motion capture: an injury biomechanics perspective"
date: 2020-09-06
categories:
  - blog

tags:
  - Motion capture
  - Deep learning
  - Injury biomechanics
  - Multibody Dynamics
---


Motion capture is the process of recording human motion, it can be used for a variety of applications including character animation, and interactive gaming in the entertainment industry, robotic motion control and planning, or biomechanical applications such as injury and performance analysis. Since the late 1900s motion capture has been used for planar (2D) biomechanical kinematic analysis, i.e. the Rotoscope [1]. Since then, a wide variety of 3D motion capture systems have been developed which allow for more detailed analysis. These can generally be divided into the categories of 1) marker-based systems, 2) wearable sensor-based systems, and 3) markerless systems. This review is focused on the recent emergence of markerless deep learning approaches, for context, and since the accuracy assessment of many of these modern approaches rely on comparisons to traditional methods, a brief introduction to marker-based, sensor-based, and markerless systems is also presented. Though body and joint positional accuracy are important considerations in all applications of motion capture, they are of notable importance for human biomechanical analysis e.g. traumatic injury biomechanics, or physiotherapy. For this reason, this review focuses on the current and potential uses of motion capture in the field of traumatic injury biomechanics. Notwithstanding the significant developments in this area driven by the aim of improving motion realism in the entertainment industry.

The operating principles of motion capture generally involve: 1) the tracking of human body keypoints which correspond to anatomical joint positions, and 2) inference of tracked keypoint locations to body kinematics by use of a defined kinematic chain i.e. a hierarchical chain of links and joints. These kinematic chains often involve the use of physically constrained joints, i.e. joints with less than 6 degrees of freedom (DOF). By taking estimates for link inertial properties i.e. body segment mass distributions, and creating a multibody system and applying inverse dynamics, the captured motion can be used to calculate dynamic information. Internal muscle response is also an important consideration and is an active research area for applications such as gait analysis, and traumatic impacts [2]. Without consideration of the internal muscular forces and torques exerted during human motion, impact loads can be estimated; this approach is commonly referred to as ragdoll physics in the animation industry, and is commonly used in biomechanical applications which do not involve a significant amount of human controlled motion, e.g. road traffic collisions (RTCs) [Pedestrian and cyclist impact : a biomechanical perspective], or sports scenarios involving impact to an unaware player [Analysis of ball carrier head motion during a rugby union tackle without direct head contact: A case study]. Injury metrics have been developed which allow predictions of injury severity by body region based on their kinematic/dynamic response in collisions [Trauma biomechanics : an introduction to injury biomechanics]. Most injury criteria are based on accelerations, relative velocities or displacements, or joint constraint forces, and need some mathematical evaluation of a time history signal. For inverse dynamics calculations, the accuracy of kinematic measurement is an important first step in biomechanical applications, as errors in joint position estimates propagate; resulting in even larger errors in dynamic estimates.

KINEMATICS IMAGE

####Traditional motion capture methods
Optical or optoelectronic motion capture systems, such as Vicon [3], OptiTrack [4], Motion Analysis [5], and Qualisys [6] involve the tracking of passive retroreflective markers placed on a human subject, using multiple calibrated high-framerate cameras (>200fps) with light emitters (usually infrared). Active marker-based systems also exist, which use LED markers in place of passive retroreflective markers, eliminating the need for light emitters [7]. These systems obtain 3D positional cartesian coordinates, i.e. 3 degrees of freedom (3DOFs) for each marker by time of flight and geometric triangulation. Vicon is the most widely used passive marker-based system and is used as a proprietary eponym. Underlying skeletal joint positions and orientations (6DOFs) can be automatically estimated by inference from external markers [8]. Joint positions are calculated using groups of markers that move in the same way, with a minimum of 3 markers needed per joint to infer 6DOFs. These joints are positioned to approximate the human skeletal structure, and their kinematic nature with differing degrees of freedom (Free Joint: 6DOF, Ball Joint: 3DOFs, Hardy Spicer Joint: 2DOFs, Hinge Joint: 1DOF) limit the axes that the links are free to rotate about; effectively reducing unrealistic motions [9]. Custom kinematic skeleton formats may also be created [10]. some marker-based systems have additional biomechanical analysis features; for example, Vicon has a plugin gait feature for calculation of lower and upper body dynamics such as joint forces, moments, and powers [11], [12]. There are also a variety of alternative motion capture systems that involve markers/sensors, including: 1) inertial systems involving the use of inertial measurement units (i.e. combined gyroscope(s), and accelerometer(s)) [13], [14], 2) mechanical systems involving the use of an instrumented exoskeleton (with sliders and potentiometers) [15], and 3) magnetic systems involving both magnetic sensor markers and a magnetic field generator [16].
Though the accuracy of many of these marker/sensor-based systems are high; they are prohibitively expensive for many, and the experimental setups are unsuitable for many in-the-wild biomechanical applications. Markerless motion capture systems allow for more natural subject motion and aim for capture in less experimentally constrained environments (i.e. outside of a motion capture laboratory) [17]â€“[20]. The first human motion capture system to be used for biomechanical analysis, the Rotoscope, invented by Max Fleisher in 1919, was in fact a markerless system. The system involves manual tracing of character outlines for individual frames of an image sequence. Most famously, this system was used by to investigate human and equine gait [1]. More recent computer-based methods now exist which employ spatially calibrated multi camera setups; streamlining the process and allowing for 3D analysis to be performed. These systems may be automatic e.g. [20],[17] or require manual annotation [21]. Systems involving both colour and depth maps (i.e. RGB-D cameras) such as the Microsoft Kinect has potential to allow for accurate single-camera motion capture, though the accuracy of the Kinect V2 is currently inadequate for many biomechanical applications. There have been efforts to improve upon the accuracy of the Kinect's skeletal tracking capabilities, all in some way involve optimizing certain parameters to achieve more realistic motions. The most successful (using 3 Kinect V1s) involved a physics-based approach utilizing body segment mass estimates from the Kinect depth sensors, insole pressure sensors, and optimization of the equations of motion [22] Another approach that achieved a similar, albeit slightly lower level of accuracy involved imposing ellipsoids on body segments based on a mesh and minimizing cross over between them, and joint rotation angle boundaries (using 2x Kinect v2s) [23]. This approach did not involve the equations of motion and did not require the use of any wearable sensors. However, the requirement of multiple RGB-D sensors makes a system such as this unsuitable for many biomechanics applications, and joint position in the order of 5cm for a number of tasks. One manual method used in the realm of biomechanical research called Multibody Image Matching (MBIM) has allowed for extraction of 3D pose from in-the-wild videos e.g. [24]. Though MBIM achieves high accuracy for certain applications [25], it is labor-intensive and are unsuitable for real-time applications. Furthermore, currently available automatic markerless systems involve a large number of calibrated cameras in controlled lab-based settings.

#### Machine learning approaches
For (near) real-time and in-the-wild applications there is a need for automatic 3D tracking of joint positions using simple inexpensive cameras. Deep learning has recently been applied in this area with great success. These approaches have allowed for robust and fast (multi) human pose estimation outside of laboratory conditions. Generally, these systems involve the training of a model with prior ground truth data. Pose estimation approaches can be categorized as 1) bottom-up approaches, which first find the keypoints and then maps them to different people in the image, and 2) top-down approaches, which use a mechanism to detect people in an image, apply a bounding box around each person, and then estimate keypoint configurations within the bounding boxes. There have been many deep learning approaches to 2D pose estimation [26]-[29]. Due to its accuracy and robustness, Openpose is widely considered to be the state of the art (SOTA) method for 2D multi-person pose estimation [26]. Openpose is a bottom-up approach that.. However, a more recent approach HRNet outperforms Openpose slightly by retaining a high-resolution image representation throughout the training process [29]. 3D pose estimation methods generally employ a 2D pose estimator as a backbone, and use techniques to project these into the 3D realm. For multi-camera setups, a simple approach involves geometric triangulation, i.e. exploiting the knowledge of camera intrinsic and extrinsic parameters of a camera to triangulate 2 views of a keypoint into 3D space [30]. A promising approach, learnable triangulation includes this geometric triangulation into the architecture, in effect the 2D joint estimates are no longer fixed, but are refined based on the 3D predictions [31]. 3D single-camera approaches have also been developed e.g. [32]-[33]. One self-supervised approach exploits epipolar geometry to obtain 3D keypoint predictions from a single viewpoint without the need for camera parameters [33]. 3D pose and shape estimation methods also exist, which allow for the inclusion of body shape predictions. Combined body pose and shape estimation is useful from an injury biomechanics perspective; with both kinematics (pose) and estimates for body segment inertia tensors (inferred from the body shape estimate) inverse dynamics can be performed, allowing for more detailed injury predictions. The most promising approaches involve the use of a statistical human body shape model (SMPL), i.e. a skinned vertex-based model that accurately represents a wide variety of body shapes in natural human poses via dimensionality reduction [34]. The current state-of the art 3D pose and shape estimation method VIBE estimates SMPL body model parameters for each frame in a video sequence using a temporal generation network, which is trained together with a motion discriminator i.e. AMASS [35].
Conclusions and future directions
The trajectory of advancement in deep learning motion capture methods has mirrored that of traditional ones. Early iterations of deep learning-based motion capture have focused on planar joint position estimates from monocular viewpoints. These systems have achieved remarkable accuracy, and consequently, more recent efforts have been pushing towards the 3D realm. The accuracy of these systems are currently well below that of gold standard systems, or other markerless systems involving depth sensors or multi-camera setups, however, this is an emerging research topic, and with the transformative effects deep learning approaches have had in other fields we can expect to see rapid improvements.

#####Sources to monitor for developments:
https://github.com.cnpmjs.org/topics/3d-pose-estimation
https://github.com.cnpmjs.org/Arthur151/SOTA-on-monocular-3D-pose-and-shape-estimation




