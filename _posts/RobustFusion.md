RobustFusion: Human Volumetric Capture with Data-driven Visual Cues using a RGBD Camera (1x Kinect V2)

High-quality and complete 4D reconstruction of human activities is critical for immersive VR/AR experience, but it suffers from
inherent self-scanning constraint and consequent fragile tracking under
the monocular setting. In this paper, inspired by the huge potential of
learning-based human modeling, we propose RobustFusion, a robust human performance capture system combined with various data-driven visual cues using a single RGBD camera. To break the orchestrated selfscanning constraint, we propose a data-driven model completion scheme
to generate a complete and fine-detailed initial model using only the
front-view input. To enable robust tracking, we embrace both the initial model and the various visual cues into a novel performance capture
scheme with hybrid motion optimization and semantic volumetric fusion,
which can successfully capture challenging human motions under the
monocular setting without pre-scanned detailed template and owns the
reinitialization ability to recover from tracking failures and the disappearreoccur scenarios. Extensive experiments demonstrate the robustness of
our approach to achieve high-quality 4D reconstruction for challenging
human motions, liberating the cumbersome self-scanning constraint.

http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123490239.pdf
